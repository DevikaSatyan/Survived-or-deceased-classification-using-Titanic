# -*- coding: utf-8 -*-
"""Titanicsurvivors.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d0rL9LlGT9dpiwpIjZ7sq_huOT3N0Qvg
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd#dataset operations
import numpy as np#multidimensional array operations
import matplotlib.pyplot as plt#plot graphs
import seaborn as sns#draw statistical graphs
#makes your plot outputs appear and be stored within the notebook
# %matplotlib inline

train = pd.read_csv('titanic_train.csv')

train.head(5)

sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='plasma')#it shows the missing data in each attributes
#cmap refers to colour map. different colour maps are available eg. magma(black and white)
#here, age and cabin (a lot are missing) has missing datas

for col in train.columns:
    print(col)

sns.set_style('whitegrid')
sns.countplot(x='Survived',data=train,palette='RdBu_r')

plt.figure(figsize=(5,5))
sns.boxplot(x='Pclass',y='Age',data=train,palette='RdBu_r')

#add missing values to age based on pclass
def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]
    
    if pd.isnull(Age):

        if Pclass == 1:
            return 37

        elif Pclass == 2:
            return 29

        else:
            return 24

    else:
        return Age

train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)

sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='plasma')

train.drop('Cabin',axis=1,inplace=True)#remove cabin attribute
#When inplace = True , the data is modified in place, which means it will return nothing and the dataframe is now updated.

train.head()

train.dropna(inplace=True)#drop NA values in other rows

train.info()

# Applying one-hot encoding
#Sex = pd.get_dummies(train['Sex'],drop_first=True)
#Embarked = pd.get_dummies(train['Embarked'],drop_first=True)
train = pd.get_dummies(train)
train.head(20)

train.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(train.drop('Survived',axis=1), 
                                                    train['Survived'], test_size=0.30, 
                                                    random_state=101)
# is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices

"""LOGISTIC REGRESSION"""

#Logistic Regression
from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
logmodel.fit(X_train,y_train)

predictions = logmodel.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

"""DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
dt.fit(X_train,y_train)

predictions=dt.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

"""RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf.fit(X_train,y_train)

predictions=dt.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))